# BERT Fine-tuning Requirements
# Core ML libraries
torch>=1.12.0
transformers>=4.20.0
datasets>=2.0.0
tokenizers>=0.12.0

# Data processing
pandas>=1.5.0
numpy>=1.21.0
scikit-learn>=1.1.0

# Training utilities
accelerate>=0.20.0
evaluate>=0.4.0
wandb>=0.13.0  # Optional: for experiment tracking

# Visualization
matplotlib>=3.5.0
seaborn>=0.11.0

# Progress tracking
tqdm>=4.64.0

# Configuration
pyyaml>=6.0